# \MODULE\-------------------------------------------------------------------------
#
#  CONTENTS      : BumbleBee
#
#  DESCRIPTION   : Nanopore Basecalling
#
#  RESTRICTIONS  : none
#
#  REQUIRES      : none
#
# ---------------------------------------------------------------------------------
# Copyright [2019] [Pay Giesselmann, Max Planck Institute for Molecular Genetics]
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Written by Pay Giesselmann
# ---------------------------------------------------------------------------------
import os, sys
from snakemake.io import glob_wildcards
from snakemake.utils import min_version


min_version("5.4.0")
configfile: "config.yaml"
localrules: tag_run, merge_run, merge_runs




# append username to shadow prefix if not present
if hasattr(workflow, "shadow_prefix") and workflow.shadow_prefix:
    shadow_prefix = workflow.shadow_prefix
    if not os.environ['USER'] in shadow_prefix:
        shadow_prefix = os.path.join(shadow_prefix, os.environ['USER'])
        print("[INFO] Shadow prefix is changed from {p1} to {p2} to be user-specific".format(
            p1=workflow.shadow_prefix, p2=shadow_prefix), file=sys.stderr)
    workflow.shadow_prefix = shadow_prefix
    print("Shadow prefix: " + shadow_prefix, file=sys.stderr)




def get_raw_batch(wildcards):
    prefix = '{raw}/{runname}/reads/{batch}'.format(
        raw=config['storage_data_raw'],
        runname=wildcards.runname,
        batch=wildcards.batch)
    if os.path.isfile(prefix + '.fast5'):
        return prefix + '.fast5'
    else:
        return prefix + '.tar'




rule segmentation:
    input:
        raw = lambda wildcards : get_raw_batch(wildcards),
        bam = "alignments/{aligner}/{sequence_workflow}/batches/{tag}/{runname}/{batch}.{reference}.bam",
        bai = "alignments/{aligner}/{sequence_workflow}/batches/{tag}/{runname}/{batch}.{reference}.bam.bai",
        genome = lambda wildcards: config['references'][wildcards.reference]['genome']
    output:
        hdf5 = "events/{aligner, [^.\/]*}/{sequence_workflow}/batches/{tag, [^\/]*}/{runname, [^\/]*}/{batch}.{reference}.hdf5"
    params:
        format = lambda wildcards, input : os.path.splitext(input.raw)[1]
    threads: 32
    resources:
        mem_mb = lambda wildcards, threads, attempt : int(threads * 2000),
        time_min = lambda wildcards, threads, attempt : int((7680 / threads) * attempt)
    shadow: 'minimal'
    shell:
        """
        /project/minion/bin/samtools view -bF 2308 {input.bam} | /project/minion/bin/bedtools bamtobed -i stdin | /project/minion/bin/bedtools getfasta -fi {input.genome} -bed stdin -name -s | sed -r 's/\(.\)//' > sequences.fa
        mkdir raw
        if [ \'{params.format}\' == '.tar' ]; then
            {config[py_env]}/bin/python3 {config[storage_fast5Index]} extract {input.raw} raw/ --output_format bulk
        else
            cp {input.raw} raw/
        fi
        {config[py_env]}/bin/python3 {config[segmentation_script]} align {config[pore_model]} sequences.fa `ls raw/*.fast5` {output.hdf5} --t {threads}
        """


rule tag_run:
    input:
        hdf5 = lambda wildcards : expand("events/{aligner}/{sequence_workflow}/batches/{tag}/{runname}/{batch}.{reference}.hdf5",
                        aligner = wildcards.aligner,
                        sequence_workflow = wildcards.sequence_workflow,
                        tag = wildcards.tag,
                        runname = wildcards.runname,
                        batch = glob_wildcards(
                            "alignments/{aligner}/{sequence_workflow}/batches/{tag}/{runname}/{{batch}}.{reference}.bam".format(
                                aligner = wildcards.aligner,
                                sequence_workflow = wildcards.sequence_workflow,
                                tag = wildcards.tag,
                                runname = wildcards.runname,
                                reference = wildcards.reference))[0],
                        reference = wildcards.reference)
    output:
        tag = temp("events/{aligner, [^.\/]*}/{sequence_workflow}/batches/{tag, [^\/]*}/{runname, [^\/]*}.{reference}.events")
    shell:
        "touch {output.tag}"


rule merge_run:
    input:
        tag = "events/{aligner}/{sequence_workflow}/batches/{tag}/{runname}.{reference}.events"
    output:
        hdf5 = "events/{aligner, [^.\/]*}/{sequence_workflow, ((?!batches).)*}/batches/{tag, [^\/]*}/{runname, [^\/]*}.{reference}.hdf5"
    params:
        input_dir = lambda wildcards, input : os.path.join(os.path.dirname(input.tag[0]), wildcards.runname)
    shell:
        """
        {config[py_env]}/bin/python3 {config[segmentation_script]} merge {output.hdf5} {params.input_dir}
        """


rule merge_runs:
    input:
        tags = lambda wildcards : ["events/{aligner}/{sequence_workflow}/batches/{tag}/{runname}.{reference}.events".format(
                aligner=wildcards.aligner,
                sequence_workflow=wildcards.sequence_workflow,
                tag=tag,
                runname=runname,
                reference=wildcards.reference)
            for tag, runnames in config['tag_map'].items() for runname in runnames]
    output:
        hdf5 = "events/{aligner, [^.\/]*}/{sequence_workflow, ((?!batches).)*}/{tag, [^\/]*}.{reference}.hdf5"
    params:
        input_dirs = lambda wildcards, input : [os.path.dirname(tag) for tag in input.tags]
    shell:
        """
        {config[py_env]}/bin/python3 {config[segmentation_script]} merge {output.hdf5} {params.input_dirs}
        """
